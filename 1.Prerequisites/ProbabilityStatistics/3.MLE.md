&emsp;
# Maximum Likelihood Estimation
- 知乎：[机器学习 逻辑（对数几率）回归算法](https://zhuanlan.zhihu.com/p/416281720)
>极大似然估计步骤
- 已知某种分布的 Distribution Function
- 计算似然函数
- 似然函数取对数
- 求最值对应参数
- 得到估计参数


两个互逆的过程
>采样
- 已知一种分布及其参数，用概率分布函数采样得到观测结果
>估计
- 已知一些数据（观测结果），对原来的参数进行估计，从而得出原来的分布


&emsp;
# 1 广义线性模型

线性回归模型为：$y = xw + b$，大家思考一下，如果我把式子改成 $lny = xw + b$ 它代表什么含义？

我们把式子变换一下：
$$e^{lny} = y = e^{xw + b}$$

，这就是”对数线性回归“，它实际上是在试图让 
 逼近y，虽然形式上仍是线性回归，但实质上已是在求输入空间到输出空间的非线性函数映射，这里的对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用。

更一般地，考虑单调可微函数g(.)，令

 （式1）

这样得到的模型称为”广义线性模型“，其中函数g(.)称为"联系函数"，显然对数线性回归是广义线性模型在g(.)=ln(.)时的特例。





&emsp;
# 2 似然函数
$X$ 为随机变量
>概率函数
$$P(X=x) = p(x | \theta_1, \theta_2, ...,\theta_k)$$
- $\theta_1, \theta_2, ...,\theta_k$: 未知参数


>抽样的概率
- 一次抽样抽到了来自总体 $X$ 的样本 $x_1, x_2, ... x_n$，事件 $X=x_i$ 的概率可以表示为：
$$P(X=x_1 | \theta_1, \theta_2, ...,\theta_k)$$

>似然函数
- 样本 $x_1, x_2, ... x_n$ 的概率函数也就是样本 $X$ 的似然函数为：
$$P(x_1, x_2, ... x_n) = \prod^{n}_{i=1}p(x_i | \theta_1, \theta_2, ...,\theta_k)$$



&emsp;
# 3 极大似然估计

<div align=center>
    <image src='imgs/normalDist.png' >
</div>

&emsp;

假设现在我们有一组采样数据：0.1, 2, 3.42, -2.3, ...，它们来自总体样本 $X\backsim N(\mu, \sigma)$，所以有，样本 $X$ 的似然函数为：
$$P(x_1, x_2, ... x_n) = \prod^{n}_{i=1}p(x_i | \mu, \sigma)$$

但是 $\mu，\sigma$ 未知，我们现在要求出这两个值怎么办？

所以问题变成了求他们似然函数的最大值
$$P= max \Big(\prod^{n}_{i=1} P(x_i | \mu, \sigma)\Big)$$

那上面这个式子好求吗？大家想想，因为概率取值范围是 $0\backsim 1$，如果大量的概率值相乘会怎么样？是不是会变得越来越小，下溢到 $0$

所以我们利用对数的性质，分别对两边求对数，将连乘变成连加
$$lnL(\mu, \sigma)  = ln\Big(\prod^{n}_{i=1} P(x_i | \mu, \sigma)\Big) = \sum\limits^n_i P(x_i | \mu, \sigma)$$

变成了求：

$$max \Big(\sum\limits^n_i P(x_i | \mu, \sigma)\Big)$$


