&emsp;
# Maximum Likelihood Estimation
- 知乎：[机器学习 逻辑（对数几率）回归算法](https://zhuanlan.zhihu.com/p/416281720)
>极大似然估计步骤
- 已知某种分布的 Distribution Function
- 计算似然函数
- 似然函数取对数
- 求最值对应参数
- 得到估计参数


两个互逆的过程
>采样
- 已知一种分布及其参数，用概率分布函数采样得到观测结果
>估计
- 已知一些数据（观测结果），对原来的参数进行估计，从而得出原来的分布


&emsp;
# 1 广义线性模型
>对数线性回归
- 线性回归模型为：$y = xw + b$，大家思考一下，如果把式子改成 $lny = xw + b$ 它代表什么含义？我们把式子两边加指数运算变换一下：
$$e^{lny} = e^{xw + b}$$
$$y = e^{xw + b}$$
- 这就是”对数线性回归“，它实际上是在试图让 $e^{xw + b}$ 逼近 $y$

&emsp;
>广义线性模型
- 更一般地，考虑单调可微函数 `g(.)`，令
    $$y = g^{-1}(w^Tx + b)$$
- 这样得到的模型称为 `广义线性模型`，其中函数 `g(.)` 称为"联系函数"，显然对数线性回归是广义线性模型在 `g(.)=ln(.)` 时的特例





&emsp;
# 2 似然函数
$X$ 为随机变量
>概率函数
$$P(X=x) = p(x | \theta_1, \theta_2, ...,\theta_k)$$
- $\theta_1, \theta_2, ...,\theta_k$: 未知参数


>抽样的概率
- 一次抽样抽到了来自总体 $X$ 的样本 $x_1, x_2, ... x_n$，事件 $X=x_i$ 的概率可以表示为：
$$P(X=x_1 | \theta_1, \theta_2, ...,\theta_k)$$

>似然函数
- 样本 $x_1, x_2, ... x_n$ 的概率函数也就是样本 $X$ 的似然函数为：
$$P(x_1, x_2, ... x_n) = \prod^{n}_{i=1}p(x_i | \theta_1, \theta_2, ...,\theta_k)$$

&emsp;
# 3 理解似然函数
假设一个函数为 $a^b$，它包含 2 个变量 a 和 b
  - 如果令 $b=2$，则得到一个关于 $a$ 的二次函数 $a^2$
  - 如果令 $a=2$，则得到一个关于 $b$ 的指数函数 $2^b$

可以发现这两个函数虽然有不同的名字，但是都源于同一个函数，那么同样，$p(x|w)$ 也是一个有 2 个变量的函数
- 如果 $w$ 为常量，则得到一个概率函数（关于 x 的函数），记为 $f(x|w)$
- 如果 $x$ 为常量，则得到一个似然函数（关于 x 的函数），记为 $L(w|x)$


&emsp;
# 4 极大似然估计

<div align=center>
    <image src='imgs/normalDist.png' >
</div>

&emsp;

假设现在我们有一组采样数据：0.1, 2, 3.42, -2.3, ...，它们来自总体样本 $X\backsim N(\mu, \sigma)$，所以有，样本 $X$ 的似然函数为：
$$P(x_1, x_2, ... x_n) = \prod^{n}_{i=1}p(x_i | \mu, \sigma)$$

但是 $\mu，\sigma$ 未知，我们现在要求出这两个值怎么办？

所以问题变成了求他们似然函数的最大值
$$P= max \Big(\prod^{n}_{i=1} P(x_i | \mu, \sigma)\Big)$$

那上面这个式子好求吗？大家想想，因为概率取值范围是 $0\backsim 1$，如果大量的概率值相乘会怎么样？是不是会变得越来越小，下溢到 $0$

所以我们利用对数的性质，分别对两边求对数，将连乘变成连加
$$lnL(\mu, \sigma)  = ln\Big(\prod^{n}_{i=1} P(x_i | \mu, \sigma)\Big) = \sum\limits^n_i P(x_i | \mu, \sigma)$$

变成了求：

$$max \Big(\sum\limits^n_i P(x_i | \mu, \sigma)\Big)$$


