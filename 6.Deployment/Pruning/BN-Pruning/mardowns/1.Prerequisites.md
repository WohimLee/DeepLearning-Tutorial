&emsp;
# Prerequisites
- VGG
- Batch Normalize


&emsp;
# 1 CIFAR10 数据集
<div align=center>
    <image src="imgs/cifar10.png" width=500></div>

- 图片数量：6w 张
- 图片宽高：32x32
- 图片类别：10
- Trainset: 5w 张，5 个训练块
- Testset: 1w 张，1 个测试块


&emsp;
# 2 VGG Net
- 官方网站：[Visual Geometry Group Home Page](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)
- 相关论文：Very Deep Convolutional Networks For Large-scale Image Recognition (2015)
>配置信息
```py
defaultcfg = {
    11 : [64,     'M', 128,      'M', 256, 256,           'M', 512, 512,           'M', 512, 512          ],
    13 : [64, 64, 'M', 128, 128, 'M', 256, 256,           'M', 512, 512,           'M', 512, 512          ],
    16 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256,      'M', 512, 512, 512,      'M', 512, 512, 512     ],
    19 : [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],
}
```

<div align=center>
    <image src="imgs/vgg16.png" width=600>
    <h4>VGG-16 网络图</h>
</div>


&emsp;
# 3 Batch Normalize
- 相关论文：Batch Normalization- Accelerating Deep Network Training b y Reducing Internal Covariate Shift (2015)
- 知乎解读：[Batch Normalization原理与实战
](https://zhuanlan.zhihu.com/p/34879333)

$$\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$

>Batch Normalize 的作用
- 加快收敛、提升精度：对输入进行归一化，从而使得优化更加容易
- 减少过拟合：可以减少方差的偏移
- 可以使得神经网络使用更高的学习率：BN 使得神经网络更加稳定，从而可以使用更大的学习率，加速训练过程
- 甚至可以减少 Dropout 的使用：因为 BN 可以减少过拟合，所以有了 BN，可以减少其他正则化技术的使用

&emsp;
>Batch Normalize 的流程
<div align=center>
    <image src="imgs/bnprocess.png" width=400>
    <h4>Batch Normalize 的流程</h>
</div>

- 最后一步是对标准化后的结果进行 "恢复"，这两个参数交给神经网络去学习


&emsp;
# 4 L1 & L2 正则
- 知乎解读：[L1 相比于 L2 为什么容易获得稀疏解？](https://www.zhihu.com/question/37096933/answer/70426653)
- 知乎解读：[L1 正则与 L2 正则的特点是什么，各有什么优势？](https://www.zhihu.com/question/26485586/answer/616029832)
- 知乎解读：[L1 正则化与 L2 正则化](https://zhuanlan.zhihu.com/p/35356992)

我们所说的正则化，就是在原来的 Loss Function 的基础上，加上了一些正则化项或者称为模型复杂度惩罚项
>Loss Function
$$L(w) = \frac{1}{N} *\sum\limits^{N}_{i=1}(y_i - w^Tx_i)^2$$

- 假设 $L(w)$ 在 0 处的导数为 0，即达到最优解：
$$\frac{\partial L(w)}{\partial w}\Bigm|_{w=0} = d = 0$$

&emsp;
## 4.1 L1 正则化（Lasso 回归）
- 加上 L1 正则项（Lasso 回归）：$C||w||_1$
- 损失函数：
    $$L_{L1}(w)= L(w) + \lambda|w|$$
- 导数：
$$\frac{\partial L(w)}{\partial w} \Bigm|_{w=0^-} = d - \lambda $$
$$\frac{\partial L(w)}{\partial w} \Bigm|_{w=0^+} = d + \lambda $$

<div align=center>
    <image src='imgs/L1.png' width=500>
</div>

&emsp;
## 4.2 L2 正则化（岭回归）
- 加上 L2 正则项（岭回归）：$C||w||^2_2$
- 损失函数：
    $$L_{L2}(w) = L(w)+ \lambda w^2$$
- 导数：
$$\frac{\partial L(w)}{\partial w} \Bigm|_{w=0}  = d + 2\lambda w =0$$

<div align=center>
    <image src='imgs/L2.png' width=500>
</div>